\documentclass{specification}
\usepackage[backend=bibtex]{biblatex}
\usepackage{dsfont}
\bibliography{bibliography}


\title{Desired Attributes of a Protein Function Description Model}
\author{Meet Barot}

\begin{document}

\maketitle

\section*{Motivations}
Why make a model that describes the common functions of a set of proteins in natural language?
\begin{enumerate}
    \item We want to be able to predict the functions of proteins, but we are limited by the amount of data that we have in both the amount of well characterized proteins and also the variety of known functions.
    \item Even the best supervised approaches can only take us to the point where we can annotate proteins that have functions that have been seen before.
    \item Explicitly ontology-based zero-shot approaches such as DeepGOZero \cite{DeepGOZero} do not allow for actual description of a new function that is discovered. The only information that is gained is that the protein has a new function that has some specified ontological relation to currently known functions. However, this may not sufficiently describe the new function, and it also excludes possible functions that do not directly relate to known functions.
\end{enumerate}
In order to discover new categories of protein function, with some amount of information to actually design experiments to test for them, we need a model that generates functional descriptions.

The following is a list of attributes we wish the model to have.

\section*{Attribute 1: Annotation correctness.}

Given a sequence set that the model is assigning scores of function descriptions:

Descriptions of GO terms that annotate the entire sequence set should be scored higher than terms that do not annotate the entire sequence set.

Let $D_{S}$ be the GO term descriptions associated with sequence set S.

\[P(d \in D_{S} | S) > P(d \notin D_{S} | S)\]

%A way to measure this attribute would be to calculate: 
%\[\frac{1}{|D_{S}|*|D_{S}^{c}|}\sum_{d_i \in D_{S}, d_j \notin D_{S}} P(d_i | S) - P(d_j | S)\]
%where $D_{S}^{c}$ is the complement of $D_{S}$.

A way to measure this attribute would be to calculate: 
\[\frac{1}{|D_{S}|*|D_{S}^{c}|}\sum_{d_i \in D_{S}, d_j \notin D_{S}} \mathds{1}(P(d_i | S) > P(d_j | S))\]
where $D_{S}^{c}$ is the complement of $D_{S}$ and $\mathds{1}$ is the indicator function.

\section*{Attribute 2: Specificity preference.}

Among terms that do annotate the whole set, the model should score child terms higher than their ancestor terms. Let $A(d)$ denote the description of a direct parent of the GO term described by $d$.

\[P(d \in D_{S}| S) > P(A(d) \in D_{S}| S)\]
Note: any protein set that is annotated with $d$ would always be annotated with $A(d)$, $A(A((d))$ and so on.

%A way to measure this attribute would be to calculate:
%\[\frac{1}{|D_{S}|}\sum_{d_i \in D_{S}} P(d_i | S) - P(A(d_i) | S)\]

A way to measure this attribute would be to calculate:
\[\frac{1}{|D_{S}|}\sum_{d_i \in D_{S}} \mathds{1}(P(d_i | S) > P(A(d_i) | S))\]


%Among terms that do annotate the whole set, the model should score more specific terms higher than less specific terms.

%Let $t(d)$ be the depth of a GO term description $d$, and $\delta$ is an arbitrary depth.
%\[P(d \in D_{S}, t(d) \geq \delta | S) > P(d \in D_{S}, t(d) < \delta | S)\]

%\section*{Attribute 3: Branch equality.}

%Among the most specific terms that annotate the whole set, descriptions from all three branches of GO should be scored equally.

%Let $B_i$ be the $i$th GO branch, and $D_{S, B_i}$ be the descriptions associated with sequence set S that are in branch $B_i$.

%\[P(d \in D_{S, B_i}, t(d) = \delta | S) = P(d \in D_{S, B_j}, t(d) = \delta | S)\]

%Problems with this: depth may not be comparable across branches or even within the same branch. Also, if a protein set is well studied within one branch but not another, it may not be correct to say that the ideal model would have the most specific terms in each branch be equally scored.

%\section*{Attribute 4: Annotation robustness.}

\section*{Attribute 3: Annotation robustness.}

Any set of sequences that have the same exact set of GO descriptions in common should produce scores with the same rankings for those GO descriptions.

Let $S_i$ and $S_j$ be different sequence sets such that $D_{S_i} = D_{S_j}$ and $S_i \neq S_j$, and let $R(X)$ be a ranking function that gives the ranks of entries in $X$, in descending order.

\[R_{d}(P(d \in D_{S_i} | S_i)) = R_{d}(P(d \in D_{S_i} | S_j))\]

A way to measure this attribute would be to calculate the average Spearman's rank correlation of the rankings for all sequence sets' correct descriptions: 

\[\frac{1}{N*(N-1)}\sum_{S_i, S_j} \frac{\textnormal{cov}(R(P(D_{S_i} | S_i)), R(P(D_{S_i} | S_j))}{\sigma_{P(D_{S_i} | S_i))}\sigma_{P(D_{S_i} | S_j))}}\]

where $N$ is the total number of sequence sets that have the exact set of GO descriptions $D_{S_i}$. In reality, this number may be too large to actually sum (especially if $|D_{S_i}|$ is small), so we would approximate this measure by subsampling $n < N$ sequence sets to average over instead. The sum is only calculated over non-identical pairs of sequence sets.

%\section*{Attribute 3: Annotation robustness.}
%
%Any set of sequences that have the same exact set of GO descriptions in common should produce the same scores for those GO descriptions.
%
%Let $S_i$ and $S_j$ be different sequence sets such that $D_{S_i} = D_{S_j}$ and $S_i \neq S_j$.
%
%\[P(d \in D_{S_i} | S_i) = P(d \in D_{S_i} | S_j)\]
%
%A way to measure this attribute would be to calculate the ratio of the probabilities, using the difference of the log probabilities: 
%
%\[\frac{1}{N*(N-1)*|D_{S_i}|}\sum_{S_i, S_j, d_{k} \in D_{S_i}} |\log P(d_k | S_i) - \log P(d_k | S_j)|\]

%where $N$ is the total number of sequence sets that have the exact set of GO descriptions $D_{S_i}$. In reality, this number may be too large to actually sum (especially if $|D_{S_i}|$ is small), so we would approximate this measure by subsampling $n < N$ sequence sets to average over instead. The sum is only calculated over non-identical pairs of sequence sets.
%\attribute*
 
%Commonality 
%
%Having a sequence in the set with a sequence that has no annotations in common with the rest of the set besides the root terms should change the scores to score only the root terms highly.
%
%Let $s$ be a protein sequence such that $D_{s} \cap D_{S} = \emptyset$.
%\[P(d \in D_{S} - D_{S, root} | S + s) = 0\]
%\[P(d \in D_{S, root} | S + s) = 1\]


% If the function description model also is given depth and branch, the desired attributes
% would probably be changed since the task is different. Kyunghyun is saying to focus on the current model and how I would evaluate it, and later additions to the training loop can be separate.

\section*{Additional Evaluation}
As these scoring metrics for evaluation are automated, they can be used for optimizing the architecture and other hyperparameters of the model (either manually or with some search method). However, in the case of actual use on proteins that are not very well studied, it can be difficult to know whether a given description is accurate. Human-assisted evaluation will be needed for the descriptions generated for a given set of novel proteins. This feedback could be used to fine-tune the model to produce more accurate, fluid or generally desirable descriptions of proteins, as has been done for document summarization models \cite{finetuningWithHuman, learningToSummarize}.

One possible way of obtaining human feedback would be to ask an expert with knowledge of the Gene Ontology and familiarity with some families of proteins to choose between two descriptions for a given sequence set that is generated from a trained model.

Doing this over a large enough dataset would allow us to train a reward estimation model that can then be used to fine-tune the original trained model using reinforcement learning. However, this would be expensive, as the task needs to be done by an expert.

\printbibliography

\end{document}

